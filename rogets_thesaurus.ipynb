{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roget's Thesaurus in the 21st Century\n",
    "\n",
    "The first known thesaurus was written in the 1st century CE by [Philo of Byblos](https://en.wikipedia.org/wiki/Philo_of_Byblos); it was called *Περὶ τῶν διαφόρως σημαινομένων*, loosly translated in English as *On Synonyms*. Fast forward about two millenia and we arrive to the most well known thesaurus, compiled by [Peter Mark Roget](https://en.wikipedia.org/wiki/Peter_Mark_Roget), a British physician, natural theologian, and lexicographer. [Roget's Thesaurus](https://en.wikipedia.org/wiki/Roget%27s_Thesaurus) was released on 29 April 1852, containing 15,000 words. Subsequent editions were larger, with the latest totalling 443,000 words. In Greek the most well known thesaurus, *Αντιλεξικόν ή Ονομαστικόν της Νεοελληνικής Γλώσσης* was released in 1949 by [Θεολόγος Βοσταντζόγλου](https://el.wikipedia.org/wiki/%CE%98%CE%B5%CE%BF%CE%BB%CF%8C%CE%B3%CE%BF%CF%82_%CE%92%CE%BF%CF%83%CF%84%CE%B1%CE%BD%CF%84%CE%B6%CF%8C%CE%B3%CE%BB%CE%BF%CF%85); the latest updated edition was released in 2008 and remains an indispensable source for writing in Greek.\n",
    "\n",
    "Roget organised the entries of the thesaurus in a hierarchy of categories. Your task in this assignment is to investigate how these categories fare with the meaning of English words as captured by Machine Learning techniques, namely, their embeddings.\n",
    "\n",
    "Note that this is an assignment that requires initiative and creativity from your part. There is no simple right or wrong answer. It is up to you to find the best solution. You have three weeks to do it. Make them count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roget's Thesaurus Classification\n",
    "\n",
    "You can find [Roget's Thesaurus classification online at the Wikipedia](https://en.wiktionary.org/wiki/Appendix:Roget%27s_thesaurus_classification). You must download the categorisation (and the words belonging in each category), save them and store them in the way that you deem most convenient for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import the needed modules for the code, We will use the following: \n",
    "\n",
    "- Requests to get the html from the site\n",
    "\n",
    "- BeautifulSoup to parse the html content \n",
    "\n",
    "- json to create the json file where we store the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will send a get request to get the site and initialize the parser for the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Send a GET request to the URL\n",
    "url = \"https://www.gutenberg.org/cache/epub/22/pg22-images.html\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to parse the content and save it as we want for the json file. I found most convinient to store it based on the pattern of Class > Divisions > Sections > Content where division exists and Class > Sections > Content where it doesn't.The Content basically contains one sentence for each word that belongs to the current section of the current division in the current class. The sentence basically is all words for the specified word in the front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chapters_data = [] # List to store the data\n",
    "\n",
    "# Find all the chapters\n",
    "\n",
    "Chapters = soup.find_all(class_=\"chapter\") # Extract all the chapters\n",
    "\n",
    "# Extract data from each chapter\n",
    "\n",
    "for chapter in Chapters: #\n",
    "    chapter_data = {} # Create a dictionary to store the data\n",
    "    chapter_data[\"Class\"] = chapter.find(\"h2\").text.strip() # Extract the class of the chapter\n",
    "    divisions = chapter.find_all(\"h2\") # Extract all the divisions\n",
    "    divisions = divisions[1:] # Remove the first division as it is the chapter\n",
    "\n",
    "    if divisions: # If there are divisions\n",
    "        chapter_data[\"Divisions\"] = [] # Create a list to store the divisions\n",
    "        for division in divisions: # Extract data from each division\n",
    "            division_data = {} # Create a dictionary to store the data\n",
    "            division_data[\"Division\"] = division.text.strip() # Extract the division\n",
    "            section_data = [] # Create a list to store the sections\n",
    "            next_sibling = division.next_sibling # Extract the next sibling\n",
    "            while next_sibling: # Extract data from each section\n",
    "                if next_sibling.name == \"h2\": # If the next sibling is a division,\n",
    "                    break # break the loop\n",
    "                if next_sibling.name == \"h3\": # If the next sibling is a section\n",
    "                    section = next_sibling.text.strip() # Extract the section\n",
    "                    paragraphs = [] # Create a list to store the paragraphs\n",
    "                    next_paragraph = next_sibling.next_sibling # Extract the next sibling\n",
    "                    while next_paragraph: # Extract data from each paragraph\n",
    "                        if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\": # If the next sibling is a section or division,\n",
    "                            break # break the loop\n",
    "                        if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"): # If the next sibling is a paragraph\n",
    "                            paragraphs.append(next_paragraph.text.strip()) # Append the paragraph to the list\n",
    "                        next_paragraph = next_paragraph.next_sibling # Move to the next sibling\n",
    "                    section_data.append({\"Section\": section, \"Content\": paragraphs}) # Append the section and its content to the list\n",
    "                next_sibling = next_sibling.next_sibling # Move to the next sibling\n",
    "            division_data[\"Sections\"] = section_data # Add the sections to the division\n",
    "            chapter_data[\"Divisions\"].append(division_data) # Add the division to the chapter\n",
    "    else: # If there are no divisions\n",
    "        section_data = [] # Create a list to store the sections\n",
    "        sections = chapter.find_all(\"h3\") # Extract all the sections\n",
    "        for section in sections: # Extract data from each section\n",
    "            section_name = section.text.strip() # Extract the section\n",
    "            paragraphs = [] # Create a list to store the paragraphs\n",
    "            next_paragraph = section.next_sibling # Extract the next sibling\n",
    "            while next_paragraph: # Extract data from each paragraph\n",
    "                if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\": # If the next sibling is a section or division,\n",
    "                    break # break the loop\n",
    "                if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"): # If the next sibling is a paragraph\n",
    "                    paragraphs.append(next_paragraph.text.strip()) # Append the paragraph to the list\n",
    "                next_paragraph = next_paragraph.next_sibling # Move to the next sibling\n",
    "            section_data.append({\"Section\": section_name, \"Content\": paragraphs}) # Append the section and its content to the list\n",
    "        chapter_data[\"Sections\"] = section_data # Add the sections to the chapter\n",
    "\n",
    "    Chapters_data.append(chapter_data) # Add the chapter to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly now with the data ready we just simply write them and store them in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to gutenberg_data.json\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Write data to JSON file\n",
    "\n",
    "with open(\"gutenberg_data.json\", \"w\") as json_file:\n",
    "    json.dump(Chapters_data, json_file, indent=4)\n",
    "\n",
    "print(\"Data has been written to gutenberg_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Embeddings\n",
    "\n",
    "You will embeddings for the word entries in Roget's Thesaurus. It is up to you to find the embeddings; you can use any of the available models. Older models like word2vec, GloVe, BERT, etc., may be easier to use, but recent models like Llama 2 and Mistral have been trained on larger corpora. OpenAI and Google offer their embeddings through APIs, but they are not free.\n",
    "\n",
    "You should think about how to store the embeddings you retrieve. You may use plain files (e.g., JSON, CSV) and vanilla Python, or a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing as always is to import the needed modules. We will use the following:\n",
    "\n",
    "- Numpy to convert our numpy array to a list\n",
    "\n",
    "- re so we can find words based on a regex\n",
    "\n",
    "- gensim for the model to make the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install gensim\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim import downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to do is get the words from the sentences in the json file and keep the ones that are in the model for the embeddings. After doing that, what is left to do is just to save the embeddings to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "File loaded.\n",
      "Words found: 199784\n",
      "Loading word2vec model...\n",
      "Finding embeddings...\n",
      "Embeddings found for 166850 words.\n",
      "Opening file...\n",
      "Embeddings stored successfully.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "print(\"Opening file...\")\n",
    "with open(\"gutenberg_data.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(\"File loaded.\")\n",
    "\n",
    "# Extract sentences from the data\n",
    "sentences = []\n",
    "for chapter in data:\n",
    "    if \"Divisions\" in chapter:\n",
    "        for division in chapter[\"Divisions\"]:\n",
    "            for section in division[\"Sections\"]:\n",
    "                for con in section[\"Content\"]:\n",
    "                    sentences.append(con)\n",
    "    else:\n",
    "        for section in chapter[\"Sections\"]:\n",
    "            for con in section[\"Content\"]:\n",
    "                sentences.append(con)\n",
    "\n",
    "# Extract words from the sentences\n",
    "words = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words.extend(re.findall(r'\\b(?![0-9]+\\b)\\w+\\b', sentence))\n",
    "\n",
    "\n",
    "print(\"Words found:\", len(words))\n",
    "\n",
    "print(\"Loading word2vec model...\")\n",
    "\n",
    "model = downloader.load(\"word2vec-google-news-300\")  # Load the word2vec model online\n",
    "\n",
    "# Load the word2vec model locally\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300', binary=True)\n",
    "\n",
    "\n",
    "print(\"Finding embeddings...\")\n",
    "\n",
    "words_in_model = []\n",
    "embeddings = []\n",
    "for word in words:\n",
    "    if word in model and word != 'N' and word !='adj' and word != \"&c\":\n",
    "        embeddings.append(model[word]) # Add the embedding to the list\n",
    "        words_in_model.append(word)\n",
    "\n",
    "print(\"Embeddings found for\", len(embeddings), \"words.\")\n",
    "\n",
    "embeddings = np.array(embeddings).tolist()  # Convert NumPy array to Python list\n",
    "\n",
    "print(\"Opening file...\")\n",
    "\n",
    "# Write embeddings to JSON file\n",
    "with open(\"embeddings.json\", \"w\") as json_file:\n",
    "    json.dump(embeddings, json_file, indent=4)\n",
    "\n",
    "print(\"Embeddings stored successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "With the embeddings at hand, you can check whether unsupervised Machine Learning methods can arrive at classifications that are comparable to the Roget's Thesaurus Classification. You can use any clustering method of your choice (experiment freely). You must decide how to measure the agreement between the clusters you find and the classes defined by Roget's Thesaurus and report your results accordingly. The comparison will be at the class level (six classes) and the section / division level (so there must be two different clusterings, unless you can find good results with hierarchical clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will import the modules needed. And these are the following:\n",
    "\n",
    "- KMeans from sklearn.cluster so we can perform the clustering \n",
    "\n",
    "- some metrics from sklearn.metrics to come in a conclusion about the agreement between the classes of Roget and the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\georg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done that, first thing we will need to do is load the embeddings that we are going to use to cluster and save each word as its class in a list so we can compare them later on with the cluster labels with the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roget's classes assigned to words.\n"
     ]
    }
   ],
   "source": [
    "# Load word embeddings\n",
    "with open(\"embeddings.json\", \"r\") as json_file:\n",
    "    embeddings = json.load(json_file)\n",
    "\n",
    "# Convert embeddings to NumPy array\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Load Roget's Thesaurus classification\n",
    "with open(\"gutenberg_data.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create a list of classes for each word based on the chapters\n",
    "rogets_classes = []\n",
    "for i, chapter in enumerate(data):\n",
    "    if \"Divisions\" in chapter:\n",
    "        for division in chapter[\"Divisions\"]:\n",
    "            for section in division[\"Sections\"]:\n",
    "                for content in section[\"Content\"]:\n",
    "                    words = re.findall(r'\\b(?![0-9]+\\b)\\w+\\b', content)  # Split content into words\n",
    "                    for word in words:\n",
    "                        if word in model and word != 'N' and word !='adj' and word != \"&c\":\n",
    "                            rogets_classes.append(i)  # Assign class index to each word\n",
    "    else:\n",
    "        for section in chapter[\"Sections\"]:\n",
    "            for content in section[\"Content\"]:\n",
    "                words = re.findall(r'\\b(?![0-9]+\\b)\\w+\\b', content)  # Split content into words\n",
    "                for word in words:\n",
    "                    if word in model and word != 'N' and word !='adj' and word != \"&c\":\n",
    "                        rogets_classes.append(i)  # Assign class index to each word\n",
    "\n",
    "print(\"Roget's classes assigned to words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are going to perform the clustering. We are going to create the same amount of clusters as the Roget's Classes, so it is going to be 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete.\n"
     ]
    }
   ],
   "source": [
    "# Cluster the word embeddings using K-means\n",
    "print(\"Clustering embeddings...\")\n",
    "num_clusters = 6 # Number of Roget's classes (chapters)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "print(\"Clustering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last but not least we are going to run some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rogets_classes: [0, 0, 0, 0, 0]\n",
      "cluster_labels: [3 3 0 0 5]\n",
      "Adjusted Rand Index: 0.027231992609547843\n",
      "Adjusted Mutual Information: 0.030546747126600788\n"
     ]
    }
   ],
   "source": [
    "# Calculate Adjusted Rand Index and Adjusted Mutual Information\n",
    "ari = adjusted_rand_score(rogets_classes, cluster_labels)\n",
    "ami = adjusted_mutual_info_score(rogets_classes, cluster_labels)\n",
    "print('rogets_classes:', rogets_classes[:5])\n",
    "print('cluster_labels:', cluster_labels[:5])\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari)\n",
    "print(\"Adjusted Mutual Information:\", ami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction\n",
    "\n",
    "Now we flip over to supervised Machine Learning methods. You must experiment and come up with the best classification method, whose input will be a word and its target will be its class, or its section / devision (so there must be two different models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "* You must submit your assignment as a Jupyter notebook that will contain the full code and documentation of how you solved the questions, plus all accompanying material, such as embedding files, etc.\n",
    "\n",
    "* You are not required to upload your assignment; you may, if you wish, do your work in GitHub and submit a link to the private repository you will be using. If you do that, make sure to share the private repository with your instructor. \n",
    "\n",
    "* You may also include plain Python files that contain code that is called by your Jupyter notebook.\n",
    "\n",
    "* You must use [poetry](https://python-poetry.org/) for all dependency management. Somebody wishing to replicate your work should be able to do so by using the poetry file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code\n",
    "\n",
    "You understand that this is an individual assignment, and as such you must carry it out alone. You may discuss with your colleagues in order to better understand the questions, if they are not clear enough, but you should not ask them to share their answers with you, or to help you by giving specific advice. You can use ChatGPT or other chatbots, if you find them useful, along with traditional web search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
