{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roget's Thesaurus in the 21st Century\n",
    "\n",
    "The first known thesaurus was written in the 1st century CE by [Philo of Byblos](https://en.wikipedia.org/wiki/Philo_of_Byblos); it was called *Περὶ τῶν διαφόρως σημαινομένων*, loosly translated in English as *On Synonyms*. Fast forward about two millenia and we arrive to the most well known thesaurus, compiled by [Peter Mark Roget](https://en.wikipedia.org/wiki/Peter_Mark_Roget), a British physician, natural theologian, and lexicographer. [Roget's Thesaurus](https://en.wikipedia.org/wiki/Roget%27s_Thesaurus) was released on 29 April 1852, containing 15,000 words. Subsequent editions were larger, with the latest totalling 443,000 words. In Greek the most well known thesaurus, *Αντιλεξικόν ή Ονομαστικόν της Νεοελληνικής Γλώσσης* was released in 1949 by [Θεολόγος Βοσταντζόγλου](https://el.wikipedia.org/wiki/%CE%98%CE%B5%CE%BF%CE%BB%CF%8C%CE%B3%CE%BF%CF%82_%CE%92%CE%BF%CF%83%CF%84%CE%B1%CE%BD%CF%84%CE%B6%CF%8C%CE%B3%CE%BB%CE%BF%CF%85); the latest updated edition was released in 2008 and remains an indispensable source for writing in Greek.\n",
    "\n",
    "Roget organised the entries of the thesaurus in a hierarchy of categories. Your task in this assignment is to investigate how these categories fare with the meaning of English words as captured by Machine Learning techniques, namely, their embeddings.\n",
    "\n",
    "Note that this is an assignment that requires initiative and creativity from your part. There is no simple right or wrong answer. It is up to you to find the best solution. You have three weeks to do it. Make them count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roget's Thesaurus Classification\n",
    "\n",
    "You can find [Roget's Thesaurus classification online at the Wikipedia](https://en.wiktionary.org/wiki/Appendix:Roget%27s_thesaurus_classification). You must download the categorisation (and the words belonging in each category), save them and store them in the way that you deem most convenient for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Step 1: Send a GET request to the URL\n",
    "url = \"https://www.gutenberg.org/cache/epub/22/pg22-images.html\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "Chapters_data = []\n",
    "\n",
    "# Find all the chapters\n",
    "\n",
    "Chapters = soup.find_all(class_=\"chapter\")\n",
    "\n",
    "# Extract data from each chapter\n",
    "\n",
    "for chapter in Chapters:\n",
    "    chapter_data = {}\n",
    "    chapter_data[\"Class\"] = chapter.find(\"h2\").text.strip()\n",
    "    divisions = chapter.find_all(\"h2\")\n",
    "    divisions = divisions[1:]\n",
    "\n",
    "    if divisions:\n",
    "        chapter_data[\"Divisions\"] = []\n",
    "        for division in divisions:\n",
    "            division_data = {}\n",
    "            division_data[\"Division\"] = division.text.strip()\n",
    "            section_data = []\n",
    "            next_sibling = division.next_sibling\n",
    "            while next_sibling:\n",
    "                if next_sibling.name == \"h2\":\n",
    "                    break\n",
    "                if next_sibling.name == \"h3\":\n",
    "                    section = next_sibling.text.strip()\n",
    "                    paragraphs = []\n",
    "                    next_paragraph = next_sibling.next_sibling\n",
    "                    while next_paragraph:\n",
    "                        if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                            break\n",
    "                        if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                            paragraphs.append(next_paragraph.text.strip())\n",
    "                        next_paragraph = next_paragraph.next_sibling\n",
    "                    section_data.append({\"Section\": section, \"Content\": paragraphs})\n",
    "                next_sibling = next_sibling.next_sibling\n",
    "            division_data[\"Sections\"] = section_data\n",
    "            chapter_data[\"Divisions\"].append(division_data)\n",
    "    else:\n",
    "        section_data = []\n",
    "        sections = chapter.find_all(\"h3\")\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            paragraphs = []\n",
    "            next_paragraph = section.next_sibling\n",
    "            while next_paragraph:\n",
    "                if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                    break\n",
    "                if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                    paragraphs.append(next_paragraph.text.strip())\n",
    "                next_paragraph = next_paragraph.next_sibling\n",
    "            section_data.append({\"Section\": section_name, \"Content\": paragraphs})\n",
    "        chapter_data[\"Sections\"] = section_data\n",
    "\n",
    "    Chapters_data.append(chapter_data)\n",
    "\n",
    "# Step 3: Write data to JSON file\n",
    "\n",
    "with open(\"gutenberg_data.json\", \"w\") as json_file:\n",
    "    json.dump(Chapters_data, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Step 1: Send a GET request to the URL\n",
    "url = \"https://www.gutenberg.org/cache/epub/22/pg22-images.html\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "Chapters_data = {}\n",
    "\n",
    "# Find all the chapters\n",
    "Chapters = soup.find_all(class_=\"chapter\")\n",
    "\n",
    "# Extract data from each chapter\n",
    "for chapter in Chapters:\n",
    "    class_name = chapter.find(\"h2\").text.strip()\n",
    "    Chapters_data[class_name] = {}\n",
    "\n",
    "    divisions = chapter.find_all(\"h2\")\n",
    "    divisions = divisions[1:]\n",
    "\n",
    "    if divisions:\n",
    "        for division in divisions:\n",
    "            division_name = division.text.strip()\n",
    "            Chapters_data[class_name][division_name] = {\"content\": []}\n",
    "\n",
    "            next_sibling = division.next_sibling\n",
    "            while next_sibling:\n",
    "                if next_sibling.name == \"h2\":\n",
    "                    break\n",
    "                if next_sibling.name == \"h3\":\n",
    "                    section = next_sibling.text.strip()\n",
    "                    paragraphs = []\n",
    "                    next_paragraph = next_sibling.next_sibling\n",
    "                    while next_paragraph:\n",
    "                        if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                            break\n",
    "                        if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                            paragraphs.append(next_paragraph.text.strip())\n",
    "                        next_paragraph = next_paragraph.next_sibling\n",
    "                    Chapters_data[class_name][division_name][\"content\"].extend(paragraphs)\n",
    "                next_sibling = next_sibling.next_sibling\n",
    "    else:\n",
    "        sections = chapter.find_all(\"h3\")\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            Chapters_data[class_name][section_name] = {\"content\": []}\n",
    "\n",
    "            next_paragraph = section.next_sibling\n",
    "            while next_paragraph:\n",
    "                if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                    break\n",
    "                if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                    Chapters_data[class_name][section_name][\"content\"].append(next_paragraph.text.strip())\n",
    "                next_paragraph = next_paragraph.next_sibling\n",
    "\n",
    "# Step 3: Write data to JSON file\n",
    "with open(\"gutenberg_data2.json\", \"w\") as json_file:\n",
    "    json.dump(Chapters_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Step 1: Send a GET request to the URL\n",
    "url = \"https://www.gutenberg.org/cache/epub/22/pg22-images.html\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "Chapters_data = {}\n",
    "\n",
    "# Find all the chapters\n",
    "Chapters = soup.find_all(class_=\"chapter\")\n",
    "\n",
    "# Extract data from each chapter\n",
    "for chapter in Chapters:\n",
    "    class_name = chapter.find(\"h2\").text.strip()\n",
    "    Chapters_data[class_name] = {}\n",
    "\n",
    "    divisions = chapter.find_all(\"h2\")\n",
    "    divisions = divisions[1:]\n",
    "\n",
    "    if divisions:\n",
    "        for division in divisions:\n",
    "            division_name = division.text.strip()\n",
    "            Chapters_data[class_name][division_name] = {}\n",
    "\n",
    "            next_sibling = division.next_sibling\n",
    "            while next_sibling:\n",
    "                if next_sibling.name == \"h2\":\n",
    "                    break\n",
    "                if next_sibling.name == \"h3\":\n",
    "                    section = next_sibling.text.strip()\n",
    "                    paragraphs = []\n",
    "                    next_paragraph = next_sibling.next_sibling\n",
    "                    while next_paragraph:\n",
    "                        if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                            break\n",
    "                        if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                            paragraphs.append(next_paragraph.text.strip())\n",
    "                        next_paragraph = next_paragraph.next_sibling\n",
    "                    Chapters_data[class_name][division_name][section] = {\"content\": paragraphs}\n",
    "                next_sibling = next_sibling.next_sibling\n",
    "    else:\n",
    "        sections = chapter.find_all(\"h3\")\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            Chapters_data[class_name][section_name] = {\"content\": []}\n",
    "\n",
    "            next_paragraph = section.next_sibling\n",
    "            while next_paragraph:\n",
    "                if next_paragraph.name == \"h3\" or next_paragraph.name == \"h2\":\n",
    "                    break\n",
    "                if next_paragraph.name == \"p\" and next_paragraph.text.strip().startswith(\"#\"):\n",
    "                    Chapters_data[class_name][section_name][\"content\"].append(next_paragraph.text.strip())\n",
    "                next_paragraph = next_paragraph.next_sibling\n",
    "\n",
    "# Step 3: Write data to JSON file\n",
    "with open(\"gutenberg_data3.json\", \"w\") as json_file:\n",
    "    json.dump(Chapters_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Embeddings\n",
    "\n",
    "You will embeddings for the word entries in Roget's Thesaurus. It is up to you to find the embeddings; you can use any of the available models. Older models like word2vec, GloVe, BERT, etc., may be easier to use, but recent models like Llama 2 and Mistral have been trained on larger corpora. OpenAI and Google offer their embeddings through APIs, but they are not free.\n",
    "\n",
    "You should think about how to store the embeddings you retrieve. You may use plain files (e.g., JSON, CSV) and vanilla Python, or a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "File loaded.\n",
      "1043\n",
      "Training Word2Vec model...\n",
      "0\n",
      "Storing embeddings...\n",
      "Embeddings stored successfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "print(\"Opening file...\")\n",
    "with open(\"gutenberg_data.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(\"File loaded.\")\n",
    "\n",
    "# check the length of the data\n",
    "#print(len(data))\n",
    "#print the first chapter\n",
    "#print(data[0])\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "for chapter in data:\n",
    "    #print(chapter[\"Class\"])\n",
    "    if \"Divisions\" in chapter:\n",
    "        for division in chapter[\"Divisions\"]:\n",
    "            #print(division[\"Division\"])\n",
    "            for section in division[\"Sections\"]:\n",
    "                #print(section[\"Section\"])\n",
    "                #print(section[\"Content\"])\n",
    "                for con in section[\"Content\"]:\n",
    "                    sentences.append(con)\n",
    "                \n",
    "    else:\n",
    "        for section in chapter[\"Sections\"]:\n",
    "            #print(section[\"Section\"])\n",
    "            #print(section[\"Content\"])\n",
    "            for con in section[\"Content\"]:\n",
    "                    sentences.append(con)\n",
    "                \n",
    "\n",
    "print(len(sentences))\n",
    "#print(sentences)\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"Training Word2Vec model...\")\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "embeddings = {}\n",
    "for word in sentences:\n",
    "    if word in model.wv:\n",
    "        embeddings[word] = model.wv[word].tolist()\n",
    "print(len(embeddings))\n",
    "\n",
    "# Store embeddings in a file (e.g., JSON)\n",
    "print(\"Storing embeddings...\")\n",
    "with open(\"rogets_embeddings.json\", \"w\") as json_file:\n",
    "    json.dump(embeddings, json_file)\n",
    "\n",
    "print(\"Embeddings stored successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "With the embeddings at hand, you can check whether unsupervised Machine Learning methods can arrive at classifications that are comparable to the Roget's Thesaurus Classification. You can use any clustering method of your choice (experiment freely). You must decide how to measure the agreement between the clusters you find and the classes defined by Roget's Thesaurus and report your results accordingly. The comparison will be at the class level (six classes) and the section / division level (so there must be two different clusterings, unless you can find good results with hierarchical clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction\n",
    "\n",
    "Now we flip over to supervised Machine Learning methods. You must experiment and come up with the best classification method, whose input will be a word and its target will be its class, or its section / devision (so there must be two different models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "* You must submit your assignment as a Jupyter notebook that will contain the full code and documentation of how you solved the questions, plus all accompanying material, such as embedding files, etc.\n",
    "\n",
    "* You are not required to upload your assignment; you may, if you wish, do your work in GitHub and submit a link to the private repository you will be using. If you do that, make sure to share the private repository with your instructor. \n",
    "\n",
    "* You may also include plain Python files that contain code that is called by your Jupyter notebook.\n",
    "\n",
    "* You must use [poetry](https://python-poetry.org/) for all dependency management. Somebody wishing to replicate your work should be able to do so by using the poetry file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code\n",
    "\n",
    "You understand that this is an individual assignment, and as such you must carry it out alone. You may discuss with your colleagues in order to better understand the questions, if they are not clear enough, but you should not ask them to share their answers with you, or to help you by giving specific advice. You can use ChatGPT or other chatbots, if you find them useful, along with traditional web search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
